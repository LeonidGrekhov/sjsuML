{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) What are the main motivations for reducing a datasetâ€™s dimensionality? - 0.5 points\n",
    "### What are the main drawbacks? - 0.5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#motivations\n",
    "#speed up the training algorithm, visualize the more important features, \n",
    "#and save space\n",
    "#drawbacks\n",
    "#some information will be lost and may decrease performance\n",
    "#adds complexity\n",
    "#computationally intensive \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset (given below) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split it into a training set and a test set\n",
    "### Take the first 60,000 instances for training, and the remaining 10,000 for testing. - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (70000, 784) \n",
      " Shape of y: (70000,)\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = mnist['data'], mnist['target']\n",
    "print('Shape of X:', X.shape, '\\n', 'Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:         pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "59995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "59996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "59997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "59998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "59999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "...        ...  ...       ...       ...       ...       ...       ...   \n",
      "59995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "59996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "59997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "59998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "59999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "0           0.0       0.0       0.0       0.0       0.0  \n",
      "1           0.0       0.0       0.0       0.0       0.0  \n",
      "2           0.0       0.0       0.0       0.0       0.0  \n",
      "3           0.0       0.0       0.0       0.0       0.0  \n",
      "4           0.0       0.0       0.0       0.0       0.0  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "59995       0.0       0.0       0.0       0.0       0.0  \n",
      "59996       0.0       0.0       0.0       0.0       0.0  \n",
      "59997       0.0       0.0       0.0       0.0       0.0  \n",
      "59998       0.0       0.0       0.0       0.0       0.0  \n",
      "59999       0.0       0.0       0.0       0.0       0.0  \n",
      "\n",
      "[60000 rows x 784 columns] \n",
      " Test Data:        pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "60000     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "60001     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "60002     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "60003     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "60004     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "60000      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "60001      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "60002      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "60003      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "60004      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "...        ...  ...       ...       ...       ...       ...       ...   \n",
      "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "60000       0.0       0.0       0.0       0.0       0.0  \n",
      "60001       0.0       0.0       0.0       0.0       0.0  \n",
      "60002       0.0       0.0       0.0       0.0       0.0  \n",
      "60003       0.0       0.0       0.0       0.0       0.0  \n",
      "60004       0.0       0.0       0.0       0.0       0.0  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "69995       0.0       0.0       0.0       0.0       0.0  \n",
      "69996       0.0       0.0       0.0       0.0       0.0  \n",
      "69997       0.0       0.0       0.0       0.0       0.0  \n",
      "69998       0.0       0.0       0.0       0.0       0.0  \n",
      "69999       0.0       0.0       0.0       0.0       0.0  \n",
      "\n",
      "[10000 rows x 784 columns] \n",
      " Train label:  0        5\n",
      "1        0\n",
      "2        4\n",
      "3        1\n",
      "4        9\n",
      "        ..\n",
      "59995    8\n",
      "59996    3\n",
      "59997    5\n",
      "59998    6\n",
      "59999    8\n",
      "Name: class, Length: 60000, dtype: uint8 \n",
      " Test Label:  60000    7\n",
      "60001    2\n",
      "60002    1\n",
      "60003    0\n",
      "60004    4\n",
      "        ..\n",
      "69995    2\n",
      "69996    3\n",
      "69997    4\n",
      "69998    5\n",
      "69999    6\n",
      "Name: class, Length: 10000, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "print('Train Data: ', X_train, '\\n', 'Test Data:', X_test, '\\n',\n",
    "     'Train label: ', y_train, '\\n', 'Test Label: ', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train a Random Forest classifier on the dataset and time how long it takes, - 1 point\n",
    "### then evaluate the resulting model on the test set. - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.96      0.97      0.96      1032\n",
      "           3       0.96      0.97      0.96      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.97      0.96      0.97       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.97      0.96      0.97      1028\n",
      "           8       0.96      0.95      0.96       974\n",
      "           9       0.95      0.95      0.95      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Report\n",
      "[[ 971    0    1    0    0    1    2    1    3    1]\n",
      " [   0 1124    2    2    0    2    2    0    2    1]\n",
      " [   6    0  998    8    2    0    2    9    7    0]\n",
      " [   0    0    7  976    0    5    0    9    9    4]\n",
      " [   1    0    2    0  955    0    4    0    2   18]\n",
      " [   3    0    1   11    3  858    7    1    5    3]\n",
      " [   7    3    2    0    2    5  938    0    1    0]\n",
      " [   1    4   20    1    2    0    0  987    1   12]\n",
      " [   4    0    5    7    5   10    4    3  927    9]\n",
      " [   7    5    0    8   12    4    1    4    6  962]]\n"
     ]
    }
   ],
   "source": [
    "pred=rf.predict(X_test)\n",
    "print (\"Classification Report\")\n",
    "print(classification_report(y_test, pred))\n",
    "print (\"Confusion Report\")\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Next, use PCA to reduce the datasetâ€™s dimensionality, with an explained variance ratio of 95%. - 2 + 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.891720</td>\n",
       "      <td>-4.929711</td>\n",
       "      <td>-0.093038</td>\n",
       "      <td>-8.064180</td>\n",
       "      <td>-0.751001</td>\n",
       "      <td>1.051980</td>\n",
       "      <td>0.283377</td>\n",
       "      <td>1.423696</td>\n",
       "      <td>-1.168523</td>\n",
       "      <td>2.406929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155843</td>\n",
       "      <td>-0.130359</td>\n",
       "      <td>0.791173</td>\n",
       "      <td>0.168495</td>\n",
       "      <td>0.157279</td>\n",
       "      <td>-0.442585</td>\n",
       "      <td>-1.471752</td>\n",
       "      <td>0.301955</td>\n",
       "      <td>0.349144</td>\n",
       "      <td>-0.752368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.813530</td>\n",
       "      <td>-7.517560</td>\n",
       "      <td>-3.714185</td>\n",
       "      <td>-1.766171</td>\n",
       "      <td>0.891472</td>\n",
       "      <td>-5.107950</td>\n",
       "      <td>-0.134795</td>\n",
       "      <td>3.197223</td>\n",
       "      <td>-0.238224</td>\n",
       "      <td>-1.003382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080121</td>\n",
       "      <td>0.143971</td>\n",
       "      <td>-0.075292</td>\n",
       "      <td>-0.054148</td>\n",
       "      <td>-0.328657</td>\n",
       "      <td>-0.097342</td>\n",
       "      <td>-0.277915</td>\n",
       "      <td>-0.066247</td>\n",
       "      <td>-0.316840</td>\n",
       "      <td>-0.180312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.204835</td>\n",
       "      <td>9.824461</td>\n",
       "      <td>-5.752488</td>\n",
       "      <td>1.479673</td>\n",
       "      <td>4.397900</td>\n",
       "      <td>2.507393</td>\n",
       "      <td>18.927843</td>\n",
       "      <td>3.888938</td>\n",
       "      <td>2.443365</td>\n",
       "      <td>-0.145296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333313</td>\n",
       "      <td>0.668359</td>\n",
       "      <td>-1.050376</td>\n",
       "      <td>-0.452535</td>\n",
       "      <td>0.122534</td>\n",
       "      <td>0.702811</td>\n",
       "      <td>1.210336</td>\n",
       "      <td>-1.227017</td>\n",
       "      <td>-0.579337</td>\n",
       "      <td>-0.585607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.534627</td>\n",
       "      <td>-4.029669</td>\n",
       "      <td>3.524017</td>\n",
       "      <td>-0.218098</td>\n",
       "      <td>5.606259</td>\n",
       "      <td>3.493731</td>\n",
       "      <td>1.445046</td>\n",
       "      <td>-4.683887</td>\n",
       "      <td>-0.613930</td>\n",
       "      <td>-2.533648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314217</td>\n",
       "      <td>-0.124090</td>\n",
       "      <td>-0.325576</td>\n",
       "      <td>-0.443792</td>\n",
       "      <td>0.208390</td>\n",
       "      <td>0.067788</td>\n",
       "      <td>-0.004242</td>\n",
       "      <td>0.064486</td>\n",
       "      <td>-0.141908</td>\n",
       "      <td>-0.005184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.251618</td>\n",
       "      <td>3.278848</td>\n",
       "      <td>-6.182219</td>\n",
       "      <td>1.462658</td>\n",
       "      <td>-1.667989</td>\n",
       "      <td>-0.580801</td>\n",
       "      <td>-0.565828</td>\n",
       "      <td>-3.129538</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>-0.972611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145473</td>\n",
       "      <td>-0.138217</td>\n",
       "      <td>0.372599</td>\n",
       "      <td>0.129113</td>\n",
       "      <td>-0.887090</td>\n",
       "      <td>0.386616</td>\n",
       "      <td>1.051650</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.359139</td>\n",
       "      <td>-0.719385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5          6    \\\n",
       "0 -0.891720 -4.929711 -0.093038 -8.064180 -0.751001  1.051980   0.283377   \n",
       "1  8.813530 -7.517560 -3.714185 -1.766171  0.891472 -5.107950  -0.134795   \n",
       "2  2.204835  9.824461 -5.752488  1.479673  4.397900  2.507393  18.927843   \n",
       "3 -6.534627 -4.029669  3.524017 -0.218098  5.606259  3.493731   1.445046   \n",
       "4 -5.251618  3.278848 -6.182219  1.462658 -1.667989 -0.580801  -0.565828   \n",
       "\n",
       "        7         8         9    ...       322       323       324       325  \\\n",
       "0  1.423696 -1.168523  2.406929  ...  0.155843 -0.130359  0.791173  0.168495   \n",
       "1  3.197223 -0.238224 -1.003382  ... -0.080121  0.143971 -0.075292 -0.054148   \n",
       "2  3.888938  2.443365 -0.145296  ...  0.333313  0.668359 -1.050376 -0.452535   \n",
       "3 -4.683887 -0.613930 -2.533648  ... -0.314217 -0.124090 -0.325576 -0.443792   \n",
       "4 -3.129538  0.005915 -0.972611  ...  0.145473 -0.138217  0.372599  0.129113   \n",
       "\n",
       "        326       327       328       329       330       331  \n",
       "0  0.157279 -0.442585 -1.471752  0.301955  0.349144 -0.752368  \n",
       "1 -0.328657 -0.097342 -0.277915 -0.066247 -0.316840 -0.180312  \n",
       "2  0.122534  0.702811  1.210336 -1.227017 -0.579337 -0.585607  \n",
       "3  0.208390  0.067788 -0.004242  0.064486 -0.141908 -0.005184  \n",
       "4 -0.887090  0.386616  1.051650 -0.171021 -0.359139 -0.719385  \n",
       "\n",
       "[5 rows x 332 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.891720</td>\n",
       "      <td>-4.929711</td>\n",
       "      <td>-0.093038</td>\n",
       "      <td>-8.064180</td>\n",
       "      <td>-0.751001</td>\n",
       "      <td>1.051980</td>\n",
       "      <td>0.283377</td>\n",
       "      <td>1.423696</td>\n",
       "      <td>-1.168523</td>\n",
       "      <td>2.406929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130359</td>\n",
       "      <td>0.791173</td>\n",
       "      <td>0.168495</td>\n",
       "      <td>0.157279</td>\n",
       "      <td>-0.442585</td>\n",
       "      <td>-1.471752</td>\n",
       "      <td>0.301955</td>\n",
       "      <td>0.349144</td>\n",
       "      <td>-0.752368</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.813530</td>\n",
       "      <td>-7.517560</td>\n",
       "      <td>-3.714185</td>\n",
       "      <td>-1.766171</td>\n",
       "      <td>0.891472</td>\n",
       "      <td>-5.107950</td>\n",
       "      <td>-0.134795</td>\n",
       "      <td>3.197223</td>\n",
       "      <td>-0.238224</td>\n",
       "      <td>-1.003382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143971</td>\n",
       "      <td>-0.075292</td>\n",
       "      <td>-0.054148</td>\n",
       "      <td>-0.328657</td>\n",
       "      <td>-0.097342</td>\n",
       "      <td>-0.277915</td>\n",
       "      <td>-0.066247</td>\n",
       "      <td>-0.316840</td>\n",
       "      <td>-0.180312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.204835</td>\n",
       "      <td>9.824461</td>\n",
       "      <td>-5.752488</td>\n",
       "      <td>1.479673</td>\n",
       "      <td>4.397900</td>\n",
       "      <td>2.507393</td>\n",
       "      <td>18.927843</td>\n",
       "      <td>3.888938</td>\n",
       "      <td>2.443365</td>\n",
       "      <td>-0.145296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668359</td>\n",
       "      <td>-1.050376</td>\n",
       "      <td>-0.452535</td>\n",
       "      <td>0.122534</td>\n",
       "      <td>0.702811</td>\n",
       "      <td>1.210336</td>\n",
       "      <td>-1.227017</td>\n",
       "      <td>-0.579337</td>\n",
       "      <td>-0.585607</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.534627</td>\n",
       "      <td>-4.029669</td>\n",
       "      <td>3.524017</td>\n",
       "      <td>-0.218098</td>\n",
       "      <td>5.606259</td>\n",
       "      <td>3.493731</td>\n",
       "      <td>1.445046</td>\n",
       "      <td>-4.683887</td>\n",
       "      <td>-0.613930</td>\n",
       "      <td>-2.533648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124090</td>\n",
       "      <td>-0.325576</td>\n",
       "      <td>-0.443792</td>\n",
       "      <td>0.208390</td>\n",
       "      <td>0.067788</td>\n",
       "      <td>-0.004242</td>\n",
       "      <td>0.064486</td>\n",
       "      <td>-0.141908</td>\n",
       "      <td>-0.005184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.251618</td>\n",
       "      <td>3.278848</td>\n",
       "      <td>-6.182219</td>\n",
       "      <td>1.462658</td>\n",
       "      <td>-1.667989</td>\n",
       "      <td>-0.580801</td>\n",
       "      <td>-0.565828</td>\n",
       "      <td>-3.129538</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>-0.972611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138217</td>\n",
       "      <td>0.372599</td>\n",
       "      <td>0.129113</td>\n",
       "      <td>-0.887090</td>\n",
       "      <td>0.386616</td>\n",
       "      <td>1.051650</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.359139</td>\n",
       "      <td>-0.719385</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5          6  \\\n",
       "0 -0.891720 -4.929711 -0.093038 -8.064180 -0.751001  1.051980   0.283377   \n",
       "1  8.813530 -7.517560 -3.714185 -1.766171  0.891472 -5.107950  -0.134795   \n",
       "2  2.204835  9.824461 -5.752488  1.479673  4.397900  2.507393  18.927843   \n",
       "3 -6.534627 -4.029669  3.524017 -0.218098  5.606259  3.493731   1.445046   \n",
       "4 -5.251618  3.278848 -6.182219  1.462658 -1.667989 -0.580801  -0.565828   \n",
       "\n",
       "          7         8         9  ...       323       324       325       326  \\\n",
       "0  1.423696 -1.168523  2.406929  ... -0.130359  0.791173  0.168495  0.157279   \n",
       "1  3.197223 -0.238224 -1.003382  ...  0.143971 -0.075292 -0.054148 -0.328657   \n",
       "2  3.888938  2.443365 -0.145296  ...  0.668359 -1.050376 -0.452535  0.122534   \n",
       "3 -4.683887 -0.613930 -2.533648  ... -0.124090 -0.325576 -0.443792  0.208390   \n",
       "4 -3.129538  0.005915 -0.972611  ... -0.138217  0.372599  0.129113 -0.887090   \n",
       "\n",
       "        327       328       329       330       331  class  \n",
       "0 -0.442585 -1.471752  0.301955  0.349144 -0.752368      5  \n",
       "1 -0.097342 -0.277915 -0.066247 -0.316840 -0.180312      0  \n",
       "2  0.702811  1.210336 -1.227017 -0.579337 -0.585607      4  \n",
       "3  0.067788 -0.004242  0.064486 -0.141908 -0.005184      1  \n",
       "4  0.386616  1.051650 -0.171021 -0.359139 -0.719385      9  \n",
       "\n",
       "[5 rows x 333 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = pd.concat([principalDf, y], axis = 1)\n",
    "finalDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05642719, 0.04041226, 0.03738273, 0.02893   , 0.02520752,\n",
       "       0.02192549, 0.01914282, 0.01740684, 0.01532232, 0.01396087,\n",
       "       0.01342175, 0.01201421, 0.01113962, 0.01090582, 0.01027986,\n",
       "       0.00994955, 0.00931255, 0.00919635, 0.008886  , 0.00863195,\n",
       "       0.00821741, 0.00798417, 0.00762573, 0.00742315, 0.0071657 ,\n",
       "       0.00689314, 0.00681399, 0.00654588, 0.00627293, 0.00610345,\n",
       "       0.00597261, 0.00589304, 0.00567358, 0.00559358, 0.00552473,\n",
       "       0.00534443, 0.00527593, 0.00515841, 0.00505498, 0.00477438,\n",
       "       0.00476312, 0.00465155, 0.00453454, 0.00445757, 0.00442313,\n",
       "       0.00437877, 0.00437294, 0.00427724, 0.00424808, 0.00418524,\n",
       "       0.00404059, 0.00396258, 0.00393176, 0.00390562, 0.00386444,\n",
       "       0.00377501, 0.00373883, 0.00368328, 0.00360377, 0.0035637 ,\n",
       "       0.00349289, 0.00344527, 0.00343239, 0.00341   , 0.00334463,\n",
       "       0.00332107, 0.00329803, 0.00319433, 0.0031711 , 0.00315431,\n",
       "       0.00309941, 0.00305782, 0.00305095, 0.0030396 , 0.00296635,\n",
       "       0.00292958, 0.00291295, 0.00290232, 0.00288608, 0.00287022,\n",
       "       0.00284514, 0.00281524, 0.00279245, 0.00278629, 0.00278233,\n",
       "       0.00276733, 0.0027542 , 0.00272901, 0.0026874 , 0.00268139,\n",
       "       0.00267354, 0.00263003, 0.00262155, 0.00258872, 0.00258429,\n",
       "       0.002571  , 0.002519  , 0.00250904, 0.00248094, 0.00244092,\n",
       "       0.00243607, 0.00241671, 0.00238999, 0.00236394, 0.00235632,\n",
       "       0.00232326, 0.00229151, 0.00225292, 0.00223891, 0.0022322 ,\n",
       "       0.00219714, 0.00219026, 0.00215303, 0.00213447, 0.00210763,\n",
       "       0.00210123, 0.00206525, 0.00205073, 0.00202906, 0.00198907,\n",
       "       0.00197597, 0.00196588, 0.00195394, 0.00194545, 0.00192834,\n",
       "       0.00191505, 0.0018926 , 0.001884  , 0.00186304, 0.0018189 ,\n",
       "       0.00180676, 0.00179009, 0.00178407, 0.00176999, 0.00175852,\n",
       "       0.0017478 , 0.00173304, 0.001728  , 0.00169433, 0.00167648,\n",
       "       0.00165855, 0.00164975, 0.00164603, 0.00164149, 0.00161138,\n",
       "       0.00160413, 0.00158927, 0.00156601, 0.00156231, 0.00154815,\n",
       "       0.00153566, 0.00152617, 0.00151562, 0.00149189, 0.0014888 ,\n",
       "       0.00147098, 0.00146274, 0.00144916, 0.00144202, 0.00142795,\n",
       "       0.00142245, 0.00142043, 0.00140016, 0.00139306, 0.00139149,\n",
       "       0.00139049, 0.00138895, 0.00138616, 0.00138451, 0.00136449,\n",
       "       0.00135787, 0.00135151, 0.001349  , 0.00133797, 0.00132635,\n",
       "       0.00131349, 0.00129857, 0.00129275, 0.00128703, 0.00127087,\n",
       "       0.00126384, 0.00125841, 0.00123504, 0.00122147, 0.00121683,\n",
       "       0.00121135, 0.00119987, 0.0011959 , 0.00117699, 0.00117216,\n",
       "       0.00116486, 0.00115194, 0.00114869, 0.00114259, 0.00113339,\n",
       "       0.00112363, 0.0011098 , 0.0010857 , 0.00108128, 0.00106264,\n",
       "       0.00105589, 0.00104707, 0.00104297, 0.00102776, 0.00101473,\n",
       "       0.00100924, 0.00099916, 0.00099336, 0.00098883, 0.00097517,\n",
       "       0.00096859, 0.00096289, 0.00095418, 0.00094574, 0.00094345,\n",
       "       0.00093592, 0.00092743, 0.00090978, 0.00090247, 0.00089122,\n",
       "       0.0008811 , 0.00087677, 0.00087115, 0.00086639, 0.0008581 ,\n",
       "       0.00085287, 0.00084026, 0.0008293 , 0.00082325, 0.00081886,\n",
       "       0.00081504, 0.00080889, 0.00080534, 0.00079901, 0.00078771,\n",
       "       0.00078122, 0.00077733, 0.00077151, 0.00075642, 0.00075139,\n",
       "       0.00074756, 0.00073904, 0.00073446, 0.00072745, 0.00072132,\n",
       "       0.00071168, 0.00070254, 0.00070072, 0.0006958 , 0.00068794,\n",
       "       0.00068262, 0.00067936, 0.00067621, 0.00066319, 0.00066042,\n",
       "       0.00065928, 0.00065277, 0.00063943, 0.00063772, 0.00063592,\n",
       "       0.0006307 , 0.00062632, 0.00061863, 0.00061657, 0.00060856,\n",
       "       0.00060434, 0.00059843, 0.00058724, 0.00058085, 0.000576  ,\n",
       "       0.00057315, 0.00057071, 0.00056459, 0.00056026, 0.00055714,\n",
       "       0.00054952, 0.0005471 , 0.00054404, 0.00054158, 0.00053849,\n",
       "       0.00053598, 0.00052652, 0.00052487, 0.00051799, 0.00051499,\n",
       "       0.00051252, 0.00050578, 0.00050238, 0.00049864, 0.00049624,\n",
       "       0.0004944 , 0.00049112, 0.00048178, 0.00047847, 0.00047651,\n",
       "       0.00047552, 0.00047147, 0.00046091, 0.00045825, 0.00044977,\n",
       "       0.00044841, 0.00044791, 0.00044299, 0.0004388 , 0.00043726,\n",
       "       0.00043261, 0.00043036, 0.0004228 , 0.00041957, 0.00041683,\n",
       "       0.00041542, 0.00041235, 0.00041092, 0.00040894, 0.00040756,\n",
       "       0.00040225, 0.00039977, 0.00039671, 0.00039165, 0.00038918,\n",
       "       0.00038757, 0.0003866 , 0.00038275, 0.00037874, 0.00037818,\n",
       "       0.00037467, 0.00037178, 0.00036996, 0.00036669, 0.00036099,\n",
       "       0.00035912, 0.00035849])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500311796713781"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 333)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:              0         1         2         3         4         5          6    \\\n",
      "0     -0.891720 -4.929711 -0.093038 -8.064180 -0.751001  1.051980   0.283377   \n",
      "1      8.813530 -7.517560 -3.714185 -1.766171  0.891472 -5.107950  -0.134795   \n",
      "2      2.204835  9.824461 -5.752488  1.479673  4.397900  2.507393  18.927843   \n",
      "3     -6.534627 -4.029669  3.524017 -0.218098  5.606259  3.493731   1.445046   \n",
      "4     -5.251618  3.278848 -6.182219  1.462658 -1.667989 -0.580801  -0.565828   \n",
      "...         ...       ...       ...       ...       ...       ...        ...   \n",
      "59995 -2.007665 -5.225704  0.158207 -4.569564 -0.484836  2.960225  -0.229372   \n",
      "59996  0.623991 -6.664259  3.325271 -8.348646 -0.806939  1.384817   3.986052   \n",
      "59997 -3.810553 -3.206503 -3.923271 -7.557723 -1.163991 -2.873995   1.709651   \n",
      "59998  1.881156 -4.918848 -0.272631  7.077436 -1.012683 -5.130376   3.903368   \n",
      "59999 -1.324238 -6.128457 -3.236615 -0.304707  3.564504  2.781371  -0.339315   \n",
      "\n",
      "            7         8         9    ...       322       323       324  \\\n",
      "0      1.423696 -1.168523  2.406929  ...  0.155843 -0.130359  0.791173   \n",
      "1      3.197223 -0.238224 -1.003382  ... -0.080121  0.143971 -0.075292   \n",
      "2      3.888938  2.443365 -0.145296  ...  0.333313  0.668359 -1.050376   \n",
      "3     -4.683887 -0.613930 -2.533648  ... -0.314217 -0.124090 -0.325576   \n",
      "4     -3.129538  0.005915 -0.972611  ...  0.145473 -0.138217  0.372599   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "59995 -3.686172 -1.572444  0.177838  ...  0.163949  0.284814 -0.402995   \n",
      "59996  0.809645 -7.317910  0.936093  ... -0.612220 -0.444122 -0.183458   \n",
      "59997 -2.500863  2.582043  4.495401  ...  0.330090  0.121593 -0.402782   \n",
      "59998  3.833569  2.743675 -0.557951  ...  0.483601  0.074958 -0.493106   \n",
      "59999 -1.950181  3.753888  2.656826  ...  0.738010  0.149280  0.494800   \n",
      "\n",
      "            325       326       327       328       329       330       331  \n",
      "0      0.168495  0.157279 -0.442585 -1.471752  0.301955  0.349144 -0.752368  \n",
      "1     -0.054148 -0.328657 -0.097342 -0.277915 -0.066247 -0.316840 -0.180312  \n",
      "2     -0.452535  0.122534  0.702811  1.210336 -1.227017 -0.579337 -0.585607  \n",
      "3     -0.443792  0.208390  0.067788 -0.004242  0.064486 -0.141908 -0.005184  \n",
      "4      0.129113 -0.887090  0.386616  1.051650 -0.171021 -0.359139 -0.719385  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "59995  0.203304 -0.341327 -0.547508 -0.485940  0.526916  0.128408 -0.406297  \n",
      "59996  0.713622 -0.109840 -0.126777 -0.237989 -0.310184  0.051893  0.107595  \n",
      "59997  0.114398 -0.302643  0.408135 -0.419265  0.372988  0.077275  0.168817  \n",
      "59998 -1.053694 -0.497977 -0.460969  0.061866  1.063966 -0.477953  0.032889  \n",
      "59999  1.049287 -0.813856 -1.058245  0.564311  0.211175  0.584211  0.259362  \n",
      "\n",
      "[60000 rows x 332 columns] \n",
      " Test Data:              0         1          2          3         4          5    \\\n",
      "60000  -5.109402  3.785687  -7.159586  -0.510415  2.856659   2.134660   \n",
      "60001   3.090641  2.326770  17.607968   3.605787  0.614082   2.906431   \n",
      "60002  -7.633542 -1.821416   3.276863   2.572342  6.675205  -2.520750   \n",
      "60003   8.466461 -1.986117  -2.684082   1.182585 -6.175808  -5.080553   \n",
      "60004   0.309111  4.763671  -3.517075   3.241124 -2.251295   0.171227   \n",
      "...          ...       ...        ...        ...       ...        ...   \n",
      "69995   3.917881  2.892419   9.949220  -2.026794 -0.893630  10.701453   \n",
      "69996   3.988380  3.381637   7.714120  -7.300283 -2.417398  -2.902458   \n",
      "69997  -5.222595 -1.184316  -4.754725   0.148113 -4.383838   4.668514   \n",
      "69998  -2.177729 -6.995033  -3.228737   1.890282  4.332407  -0.846104   \n",
      "69999  11.777872 -5.402981   2.322725  12.295654 -7.546739  -4.651582   \n",
      "\n",
      "            6         7         8         9    ...       322       323  \\\n",
      "60000 -3.672991  7.009075 -4.043855  2.320864  ... -0.549713  0.223378   \n",
      "60001 -3.724699  5.693068  4.804261  4.715735  ...  0.197590 -0.071570   \n",
      "60002  1.529153 -0.841633 -0.810574 -0.544461  ... -0.037109 -0.340147   \n",
      "60003 -3.490617  6.129373 -0.549641 -2.062357  ...  0.207555 -0.424987   \n",
      "60004  3.444357 -4.617760 -4.816294  3.848351  ... -0.117284  0.225703   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "69995 -5.194376  1.941835 -4.810929 -3.655669  ... -0.662055 -0.295145   \n",
      "69996 -1.120196  6.798138 -2.166906  0.092647  ...  0.129393  0.178405   \n",
      "69997 -5.142121 -1.334809  0.659764  0.548709  ...  0.475021  0.154670   \n",
      "69998 -3.363003  1.797760  9.549834  2.533057  ...  0.927025 -1.633472   \n",
      "69999  0.452593  3.132267  5.070676 -6.958615  ... -0.541411  0.263911   \n",
      "\n",
      "            324       325       326       327       328       329       330  \\\n",
      "60000 -0.168534 -0.344058  0.029815  0.262610 -0.282002 -0.481080  0.354805   \n",
      "60001 -0.391016  0.637199  0.632456 -0.209942 -0.304608 -0.009903  0.226608   \n",
      "60002 -0.181095 -0.321980 -0.076249  0.111806  0.137228 -0.113142  0.075821   \n",
      "60003  0.384517  0.123058 -0.492113 -0.408247  0.208853  0.244852  0.015082   \n",
      "60004  0.719572  0.349900 -0.433191 -0.377573 -0.077205 -0.137154  0.083836   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "69995  0.131727 -0.245305 -0.103730 -0.197661 -1.215230  0.094818 -0.080631   \n",
      "69996 -0.273562 -0.069066 -0.604797  0.089073  0.339831  0.154674 -0.341584   \n",
      "69997 -0.409326  0.336293 -0.252122 -0.457563  0.155719 -0.128802  0.157049   \n",
      "69998  0.952766  0.249581  0.126999  0.574815 -1.428010  0.350582 -0.384378   \n",
      "69999 -0.275874 -0.509869  0.029141 -0.801903  0.761732  0.110692  0.153549   \n",
      "\n",
      "            331  \n",
      "60000  0.206588  \n",
      "60001  0.537321  \n",
      "60002 -0.135237  \n",
      "60003 -0.026821  \n",
      "60004 -0.822395  \n",
      "...         ...  \n",
      "69995 -0.178934  \n",
      "69996  0.263532  \n",
      "69997 -0.154440  \n",
      "69998  0.310564  \n",
      "69999  0.118261  \n",
      "\n",
      "[10000 rows x 332 columns] \n",
      " Train label:  0        5\n",
      "1        0\n",
      "2        4\n",
      "3        1\n",
      "4        9\n",
      "        ..\n",
      "59995    8\n",
      "59996    3\n",
      "59997    5\n",
      "59998    6\n",
      "59999    8\n",
      "Name: class, Length: 60000, dtype: uint8 \n",
      " Test Label:  60000    7\n",
      "60001    2\n",
      "60002    1\n",
      "60003    0\n",
      "60004    4\n",
      "        ..\n",
      "69995    2\n",
      "69996    3\n",
      "69997    4\n",
      "69998    5\n",
      "69999    6\n",
      "Name: class, Length: 10000, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = principalDf[:60000], principalDf[60000:], y[:60000], y[60000:]\n",
    "print('Train Data: ', X_train, '\\n', 'Test Data:', X_test, '\\n',\n",
    "     'Train label: ', y_train, '\\n', 'Test Label: ', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)  Train a new Random Forest classifier on the reduced dataset and see how long it takes. - 1 point\n",
    "### Was training much faster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing the two training times, the first clasifier took 36.7 seconds to train \n",
    "#while the new classifier took 1min 57s to train increasing computation time significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Next evaluate the classifier on the test set: how does it compare to the previous classifier? - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       980\n",
      "           1       0.99      0.98      0.99      1135\n",
      "           2       0.92      0.92      0.92      1032\n",
      "           3       0.89      0.93      0.91      1010\n",
      "           4       0.94      0.95      0.95       982\n",
      "           5       0.94      0.90      0.92       892\n",
      "           6       0.95      0.96      0.96       958\n",
      "           7       0.93      0.92      0.92      1028\n",
      "           8       0.93      0.92      0.92       974\n",
      "           9       0.93      0.90      0.92      1009\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n",
      "Confusion Report\n",
      "[[ 958    0    4    3    0    3    8    2    2    0]\n",
      " [   0 1116    6    2    2    1    5    0    3    0]\n",
      " [  13    0  953   20    4    0    9    9   23    1]\n",
      " [   2    0   15  941    1   13    1   17   15    5]\n",
      " [   0    0    6    1  932    1    8    7    4   23]\n",
      " [   6    0    4   42    5  805   11    7    8    4]\n",
      " [   7    2    8    1    4   12  921    0    3    0]\n",
      " [   1    6   21   10    9    1    0  947    3   30]\n",
      " [   9    0    9   23    4   17    4    7  892    9]\n",
      " [   7    4    5   17   27    4    0   26    7  912]]\n"
     ]
    }
   ],
   "source": [
    "pred=rf.predict(X_test)\n",
    "print (\"Classification Report\")\n",
    "print(classification_report(y_test, pred))\n",
    "print (\"Confusion Report\")\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the new clacifier precision falls to 94% while on the full dataset the precision was above 97% average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
